# Cluster-Analysen im SQL-Notebook und Visualisierung mit Calculation Views

Mit dem SQL-Notebook im Business Application Studio kann man interaktiv SQL-Statements oder Bl√∂cke von SQLScript ausf√ºhren. Das l√§sst sich vielf√§ltig nutzen, etwa zur Datenanalyse, als interaktive Read-Me-Datei oder f√ºr Setup-Instruktionen eines SAP-HANA-Projektes in CAP.
In diesem Blog zeige ich die Ausf√ºhrung einer Cluster-Analyse in einem SQL-Notebook. Die Ergebnisse werden in einer Tabelle gespeichert und k√∂nnen anschlie√üend mit Calculation Views visualisiert werden. Anschlie√üend skizziere ich die Verwendung von HDI-Containern und die Rolle des Space-Konzeptes von SAP HANA im Kontext von Machine Learning.


<!-- Start Document Outline -->

* [SQL-Notebook](#sql-notebook)
   * [Cluster-Analyse in SQLScript](#cluster-analyse-in-sqlscript)
   * [Statistische Berechnungen](#statistische-berechnungen)
   * [Einschub: KI-gest√ºtzte Intepretation](#einschub-ki-gest√ºtzte-intepretation)
* [Visualisierung in Calculation Views](#visualisierung-in-calculation-views)
* [Architektonische Betrachtungen - Data Spaces und HDI Container in SAP HANA](#architektonische-betrachtungen---data-spaces-und-hdi-container-in-sap-hana)
* [Zusammenfassung](#zusammenfassung)
* [Referenzen](#referenzen)

<!-- End Document Outline -->




Zun√§chst m√∂chte ich das Vorgehen motivieren:

- *SQL-Notebook als Erweiterung der klassischen SQL-Konsole*: Im SQL-Notebook k√∂nnen die Ausgaben von mehreren Befehlen gespeichert werden, w√§hrend man in der SQL-Konsole immer nur das Ergebnis der letzten Ausf√ºhrung sieht.
- *SQLScript f√ºr Datenanalysten*: Datenanalysten arbeiten gerne mit SQLScript (neben Python). Das SQL-Notebook erlaubt es, verschiedene Analyse-Skripte strukturiert und gegliedert in einer Datei im Code-Repository zu speichern - ohne Prozeduren oder Tabellenfunktionen anlegen zu m√ºssen. 
- *Zusammenspiel mit Calc Views*: Mit Calculation Views und der Datenvorschau im Database Explorer (oder einem BI-Tool) k√∂nnen vielf√§ltige  Visualisierungen des Ergebnis der Datenanalyse erstellt werden.

## SQL-Notebook

F√ºr die Nutzung eines SQL-Notebooks ben√∂tigen Sie ein SAP-HANA-Projekt im Business Application Studio (oder in SAP Build). Ein SQL-Notebook wird als Datei mit der Endung "notebook" angelegt.

Ein SQL-Notebook besteht aus mehreren Bl√∂cken verschiedenen Typs:

- Connection Management: Zum Anzeigen der verf√ºgbaren Verbindungen und Ausw√§hlen einer Verbindung zur HANA-Datenbank
- Markdown-Block: Enth√§lt Text in Markdown. F√ºr √úberschriften oder Erl√§uterungen
- SAP HANA SQL: Enth√§lt SQL- oder SQLScript-Anweisungen und erlaubt deren Ausf√ºhrung. 

Zun√§chst wird ein Block vom Typ Connection Management angelegt. In diesem Block wird der Befehl `#List database connections` eingetragen. Nach Ausf√ºhrung werden die verf√ºgbaren Verbindungen angezeigt. Das SQL-Notebook liest hierzu die Verbindungen, die dem Cloud-Foundry-User zugewiesen sind, aus:

![10 SQL-Notebook HANAConnectionManagement](10_SQLNotebook_HANAConnectionManagement.png)

Dies sind die gleichen Verbindungen, die Ihrem Cloud-Foundry-Nutzer im Database Explorer der HANA Cloud zugewiesen sind.

In unserem Szenario sieht man:
- zwei Verbindungen zu HDI-Containern (erkennbar an *SharedDevKey*),
- eine on-Premise-Verbindung zu HANA Express
- eine native Verbindung zur HANA Cloud (mit Datenbankuser *DBADMIN* ohne HDI-Container). 
Mit Auswahl einer Verbindung wird die Verbindung zur HANA aufgebaut.

:nerd_face: Auch wenn das SQL-Notebook ein Cloud-Tool ist, kann man sich zu einer on-Premise HANA-Instanz verbinden.

Zur Best√§tigung kann man mit dem Befehl `#Show current database connection` die aktuelle Verbindung zur HANA anzeigen:

![20 SQL-Notebook HANAConnectionManagement 2](20_SQLNotebook_HANAConnectionManagement_2.png)

Jetzt kann man loslegen und erste SQL-Befehle ausf√ºhren. F√ºr die Cluster-Analyse verwende ich - wie auch in meinem Buch - die Datenmenge CHURN (siehe [^churn]). Diese enth√§lt 10.000 Datens√§tze von fiktiven Kunden einer Bank. Urspr√ºnglich gedacht f√ºr die Entwicklung eines Prognosemodells zum K√ºndigungsverhalten, verwende ich diese hier zur Veranschaulichung einer Cluster-Analyse.

Beginnen wir mit einem einfachen Select-Statement. F√ºgen Sie hierzu einen Block vom Typ "SAP HANA SQL" ein:

```sql
SELECT * FROM CHURN
```

Als Ergebnis erh√§lt man die Tabelle beschr√§nkt auf die ersten Zeilen:

![30 SQL-Notebook SelectCHURN](30_SQLNotebook_SelectCHURN.png)

M√∂chte man eine zuf√§llige Stichprobe ausw√§hlen, kann man auch den Befehl erg√§nzen:

```sql
SELECT * FROM CHURN ORDER BY rand() limit 5
```
![40 SQLNotebook SelectCHURN Limit Sample](40_SQLNotebook_SelectCHURN_Limit_Sample.png)

### Cluster-Analyse in SQLScript

Auf Basis der Tabelle CHURN f√ºhren wir nun die Cluster-Analyse durch. Mit einer Cluster-Analyse werden die Kunden segmentiert in Gruppen √§hnlicher Kunden.
F√ºr die Cluster-Analyse verwende ich den K-Means-Algorithmus, bereit gestellt durch die PAL-Prozedur `_SYS_AFL.PAL_KMEANS`.
Das Vorgehen ist drei geteilt:

- Aufbau der Parameter-Tabelle f√ºr die KMeans-Prozedur
- Aufruf der Prozedur
- Speichern der Cluster-Zuweisung in einer (container-lokalen) Tabelle

Diese Logik muss nun in einem anonymen Block gekapselt werden. Ich verwende hier den Quellcode aus meinem Buch "Machine Learning mit SAP HANA" (Espresso Tutorial, 2022, [^thebook]). F√ºr die technischen Details empfehle ich die Lekt√ºre meines Buches.

Betrachten wir den Ausschnitt zum Aufbau der Parameter-Tabelle:
```sql
DO BEGIN
-- Auslassung: Definition von :lt_parameter 
...

INSERT INTO :lt_parameter VALUES ('GROUP_NUMBER', 5, NULL, NULL);
INSERT INTO :lt_parameter VALUES ('INIT_TYPE', 1, NULL, NULL);
INSERT INTO :lt_parameter VALUES ('DISTANCE_LEVEL',2, NULL, NULL);
INSERT INTO :lt_parameter VALUES ('THREAD_RATIO', NULL, 0.1, NULL);

-- Zu Testzwecken: Ausgabe
SELECT * FROM :lt_parameter;

END;
```

Ausf√ºhrung liefert:
![50 SQLNotebook ParameterTable](./50_SQLNotebook_ParameterTable.png)

Mit einem SELECT-Statement selektieren wir die relevanten Spalten, die als Eingabe-Features f√ºr das Clustering gew√§hlt werden sollen. Unterschiedliche Auswahl der Features f√ºhrt zu unterschiedlichen Cluster-Ergebnissen.
Zugleich erweitern wir den anonymen Block um den Aufruf der Cluster-Prozedur:
```sql
lt_churn = SELECT CUSTOMERID,
				  GEOGRAPHY,
				  AGE,
				  TENURE,
				  BALANCE,
				  NUMOFPRODUCTS,
				  ESTIMATEDSALARY
				  FROM CHURN WHERE BALANCE > 0;


CALL _SYS_AFL.PAL_KMEANS(:lt_churn, 
                          :lt_parameter, 
                          lt_result, 
                          lt_centers, 
                          lt_model, 
                          lt_statistics, 
                          lt_placeholder);
```
Die Cluster-Zuweisung wird in ```lt_result``` ausgegeben, die Cluster-Zentren in ```lt_centers```.
Durch das Anf√ºgen eines Select-Befehls im anonymen Block wird nach Ausf√ºhrung die Cluster-Zuweisung ausgegeben:
![60 SQLNotebook ClusterResult](./60_SQLNotebook_ClusterResult.png)

Um die Cluster-Zuweisung f√ºr weitere Analysen zu verwenden, speichern wir diese nach Ausf√ºhrung in der (container-lokalen) Tabelle ```CLUSTER_RESULT```.

:üí° : Unterschied Python und SQL-Notebook

Anders als im Jupyter Notebook f√ºr Python, werden in einem SQL-Notebook keine lokalen Variablen definiert. Die Bl√∂cke bilden voneinander unabh√§ngige Einheiten, die nur die gemeinsame Datenbankverbindung nutzen.
Ein Informationsaustausch zwischen den Bl√∂cken ist somit nur √ºber Session-Variablen oder tempor√§re Tabellen m√∂glich. In dem hier gezeigten Beispiel verwende ich eine persistente Tabelle ```CLUSTER_RESULT```, die ich in einem HANA-Projekt als *hdbtable*-Datei definiert habe. Diese wird dann als container-lokale Tabelle deployed.

### Statistische Berechnungen 
Um die Verteilung der numerischen Attribute nach Cluster genauer zu analysieren, kann man Mittelwerte und  Perzentile berechnen. Die Perzentile lassen sich in SQLScript als Window-Expression mit der Window-Aggregations-Funktion ```percentile``` berechnen.
Der folgende Code-Block selektiert zun√§chst das Ergebnis der Cluster-Analyse mit ```RUN_ID = 1 ``` und f√ºhrt dann die Percentile-Berechnung durch:
```sql
DO BEGIN 

lt_customer_cluster = SELECT c.customerid, 
                             r.cluster_id, 
                             c.age, 
                             c.balance FROM CHURN as c JOIN cluster_result as r on c.CUSTOMERID = r.CUSTOMERID
                             where r.RUN_ID = 1;

lt_percentile = SELECT customerid, cluster_id, age, balance,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY age) OVER (PARTITION BY cluster_id) AS age_p25,
    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY age) OVER (PARTITION BY cluster_id) AS age_p50, 
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY age) OVER (PARTITION BY cluster_id) AS age_p75, 
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY balance) OVER (PARTITION BY cluster_id) AS balance_p25,
    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY balance) OVER (PARTITION BY cluster_id) AS balance_p50,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY balance) OVER (PARTITION BY cluster_id) AS balance_p75 from :lt_customer_cluster;
     

SELECT cluster_id, max(age_p25) as age_p25, 
                   max(age_p50) as age_p50,
                   max(age_p75) as age_p75, 
                   avg(age) as mean_age,
                   max(balance_p25) as balance_p25, 
                   max(balance_p50) as balance_p50, 
                   max(balance_p75) as balance_p75,
                   avg(balance) as mean_balance
                   FROM :lt_percentile group by cluster_id order by cluster_id;


END;
```

Ergebnis:
![70 SQLNotebook WindowFunctionPercentile](./70_SQLNotebook_WindowFunctionPercentile.png)

üí°  Die Percentile-Cont-Funktion ist eine sogenannte *Window Aggregate function*: Innerhalb der definierten Partition (hier die Cluster) wird eine Aggregation berechnet und der aggregierte Wert an jede Zeile der Partition geschrieben. Die Percentile-Funktionen stehen aber nicht bei einer gew√∂hnlichen Aggregation mit einem ```Group By```-Statement zur Verf√ºgung.

- Das Ergebnis l√§sst sich folgenderma√üen interpretieren: Die Altersverteilung unterscheidet sich nicht zwischen den Clustern (die mittleren 50% liegen im Bereich 32 - 45). 
- Die Verteilung f√ºr Balance (Kontostand) unterscheidet sich zwischen den Clustern:
  - Cluster *0* und *4* enthalten die Kunden mit niedrigeren Kontost√§nden.
  - Cluster *1* und *2* enthalten Kunden mit h√∂heren Kontost√§nden.

Andere Segmentierungen der Kunden erh√§lt man durch die Reduktion der Eingabedaten auf bestimmte Features oder die Verwendung einer Normalisierung der Features. Im SQL-Notebook (siehe das GitHub-Projekt: [^git] ) ist eine weitere Cluster-Konfiguration aufgef√ºhrt, bei der nur nach Alter und Creditscore segmentiert wird und zus√§tzlich die Eingabevariable vorher normalisiert werden. 

### Einschub: KI-gest√ºtzte Intepretation

Beim Schreiben dieses Blogs habe ich - mehr zum Spa√ü - die nach Cluster aggregierte Ausgabe des vorherigen Abschnitt in Microsoft Copilot als Prompt kopiert. Das kam dabei heraus:

ü§ì Die tabellarische Ausgabe der statistischen Kennzahlen k√∂nnen Sie mittels der Funktion "Copy Cell Output" als JSON-Array kopieren. Diese Zeichenkette k√∂nnen Sie 1:1 in Microsoft Copilot eingeben. Microsoft Copilot f√ºhrt dann eigenst√§ndig eine Interpretation der Cluster-Ergebnisse (auf Basis der zusammengefassten statistischen Kennzahlen) durch und erlaubt sogar die Visualisierung der Cluster. 


![71 SQLNotebook CopyToClipboard](71_SQLNotebook_CopyToClipboard.png)

Aufbereitung durch Copilot/ChatGPT:


![72 SQLNotebook Copilot Intepretation](72_SQLNotebook_Copilot_Intepretation.png)

Der Clou: Die statistischen Berechnungen finden bereits (durch Ausf√ºhrung des SQL-Statements oben) in der SAP HANA statt  - dank In-Memory pfeilschnell. Copilot muss hier nur die berechneten statistischen Kennzahlen verarbeiten, dies entspricht einer hoch aggregierten und folglich sehr kleinen Datenmenge. Insbesondere m√ºssen keine individuellen Kundens√§tze aus dem SAP-System an Copilot geliefert werden.





## Visualisierung in Calculation Views

Nach dem wir nun via SQLScript die Cluster-Analyse ausgef√ºhrt haben, l√§sst sich diese mittels Calculation Views weiter untersuchen und visualisieren.

In meinem letzten Blog[^blog_calc_views] habe ich bereits einige Visualisierungen gezeigt:

- Blasendiagramm f√ºr die Cluster-Zentren
- Streudiagramm mit zuf√§lliger Stichprobe pro Cluster

Die grundlegende Modellierung besteht darin, die Quelltabelle CHURN (mit den Kundendaten) durch einen JOIN mit der Ergebnistabelle f√ºr die Clusterzuweisungen *CLUSTER_RESULT* zu verkn√ºpfen.

Die folgende Abbildung zeigt die Grundform des Calculation Views:

![80 CalculationView JOIN](./80_CalculationView_JOIN.png)

Visualisierung der Cluster-Zentren als Blasendiagramm:

![90 CalculationView BubbleChart](./90_CalculationView_BubbleChart.png)

Streudiagramm mit zuf√§lliger Stichprobe pro Cluster:

![100 CalculationView BubbleChart](./100_CalculationView_BubbleChart.png)

Wie man die zuf√§llige Stichprobe mittels dem Knotentyp Window Function modelliert, habe ich im  letzten Blog[^blog_calc_views] beschrieben.

Ich m√∂chte nun eine weitere hilfreiche Technik f√ºr die  Analyse vorstellen, das sogenannte *Binning*: Die Spalte Alter (*AGE*) l√§sst sich durch die Window Function *NTILE* diskretisieren, d.h. eine kategoriale Spalte erstellen, die jeden Datensatz einem Intervall auf Basis des Alters bzw. jeden Kunden einer Altersgruppe zuweist. Das l√§sst sich nutzen, um die Verteilung der Kunden in den einzelnen Clustern nach Altersgruppe zu analysieren.

F√ºr die Erstellung der diskreten Spalte *AGE_BIN* erstelle ich den Dimension View *CV_D_AGE_BINNING*, der folgenden Aufbau hat:

![110 CalculationView BubbleChart](./110_CalculationView_BubbleChart.png){height = 500px}

Die einzelnen Knoten haben folgende Funktion:

- Aggregationsknoten A_REMOVE_DUPLICATE_AGE: Durch Aggregation auf der Spalte AGE (als Aggregationsmerkmal) werden die distinkten Werte der Spalte AGE selektiert. Dies entspricht im SQL dem *SELECT DISTINCT*.
- Window Function Knoten W_AGE_BIN: Verwendung der Funktion *N_TILE* zum Unterteilen des Wertebereichs von *AGE* in Intervalle. Jeder Wert f√ºr AGE wird einer Gruppe zugewiesen. Diese Zuweisung bildet die neue Spalte *AGE_BIN*.
- Window Function Knoten W_AGE_MIN_MAX: Berechnet pro Gruppe (*AGE_BIN*) das minimale und maximale Alter.
- Projection: Ausgabe der Spalten und zus√§tzliche Berechnung der Intervallbeschriftung *AGE_LABEL*.

Die Modellierung der Window Function im Knoten *W_AGE_BIN* sieht so aus:

![120 CalculationView Age Bin WindowFunction](./120_CalculationView_Age_Bin_WindowFunction.png)

![121 CalculationView Age Bin WindowFunctionOrder](./121_CalculationView_Age_Bin_WindowFunctionOrder.png)

- Die Datens√§tze werden nach AGE sortiert (im Reiter *Partition and Order* einstellen).
- Die Window Function *Ntile* bildet dann Gruppen der Datens√§tze entsprechend der Sortierung.
- Die Anzahl der zu bildenden Gruppen wird √ºber die Einstellung *number_of_buckets* definiert. Diese versorgen wir √ºber den Eingabeparameter *IP_NUMBER_OF_BINS*.

Eine Beispielausgabe f√ºr *IP_NUMBER_OF_BINS = 10* sieht so aus:

![130 CalculationView Age Bin Preview](./130_CalculationView_Age_Bin_Preview.png)

Nun l√§sst sich der zuvor modellierte Cube View (Verkn√ºpfung von Kundenattributen und Cluster-Zuweisung) um einen Join mit dem Dimension View *CV_D_AGE_BINNING*  erweitern. 
Die Anzahl der Kunden l√§sst sich nun pro Cluster und Altersgruppe z√§hlen.
Wir betrachten die Visualisierung f√ºr *IP_NUMBER_OF_BINS = 5*.

Beispielsweise vergleicht das folgende Balkendiagramm die Altersverteilung in den Clustern *0*,*1* und *3*:

![140 CalculationView Cluster Age Bin](./140_CalculationView_Cluster_Age_Bin_.png)

Erkennbar ist, dass in Cluster *0* die Altersgruppen *46-59*, *60-73* und *74-92* vertreten sind, in den Cluster *1* und *3* hingegen die Altersgruppen *18-31* und *32-45* dominieren.

Zum Vergleich kann man auch die Altersverteilung √ºber die Gesamtheit der Kunden hinweg betrachten:

![145 CalculationView Total Age Bin](./145_CalculationView_Total_Age_Bin_.png)

## Architektonische Betrachtungen - Data Spaces und HDI Container in SAP HANA

In diesem Kapitel m√∂chte ich darauf eingehen, warum die Nutzung von Data Spaces und HDI Containern in SAP HANA hilfreich f√ºr den erfolgreichen Einsatz von Machine Learning ist.

Der Prozess zur Entwicklung von Machine-Learning-Modellen bzw. ML-gest√ºtzten Datenanalysen ben√∂tigt u.a. zwei Rahmenbedingungen:

- Zugriff auf produktive Echt-Daten - ggf. anonymisiert und historisiert.
- M√∂glichkeit zum Erstellen von HANA-Artefakten (Calculation Views, Stored Procedures, etc.) und dem Speichern von Zwischenergebnissen.

Die klassische Drei-System-Umgebung einer SAP-Landschaft aus Entwicklungssystem, Testsystem und Produktion ist f√ºr Machine-Learning daher nur mit Einschr√§nkungen geeignet. 
Hier kommt die Nutzung von Spaces ins Spiel, die entweder nativ in SAP HANA oder in Datasphere angelegt werden k√∂nnen.
Ein Space ist eine isolierte Umgebung in HANA mit eigener Berechtigungsverwaltung und Steuerung der Ressourcennutzung. Beim Deployment eines HDI-Container w√§hlt man aus, in welchen Space der Container deployed werden soll.

Die folgende Abbildung zeigt die Nutzung von Spaces im Kontext von Machine Learning:

![110 CalculationView BubbleChart](./150_Hana_Spaces.png)

Auf einem Produktionsystem kann ein eigener "Green space" f√ºr Machine Learning eingerichtet werden.
Dem Data Scientist bzw. Machine-Learning-Entwickler werden entsprechende Berechtigungen in diesem Green Space zugewiesen. 
Der blaue Kasten "Customer Data" symbolisiert hier die Kundendaten, die als Eingabedaten f√ºr die Cluster-Analyse verwendet werden.
Der Zugriff auf bereits vorhandene Daten wird durch entsprechende *External Services* bereit gestellt. Dies entspricht im Prinzip einem technischen Datenbankuser.
Durch die Beschr√§nkung der CPU- und Speichernutzung des Green spaces wird verhindert, dass das Produktionssystem zu stark durch die Ausf√ºhrung der Machine-Learning-Algorithmen belastet wird.

F√ºr die Entwicklung von Datenbank-Artefakten sind die HDI-Container (in XSA oder HANA Cloud) ideal: 
- Jeder ML-Entwickler kann seine Entwicklung zun√§chst in einem eigenen Container deployen. 
- Zwischenergebnisse werden in Container-lokalen Tabellen gespeichert.

Das Zusammenf√ºhren von Entwicklungen kann dann zentral √ºber ein Git-Repository erfolgen.

Sobald ein geeignetes Analyse-Vorgehen - bestehend aus Datenvorbereitung, Selektion der relevanten Features und einem geeignetem Cluster-Algorithmus - gefunden wurde, kann die Machine-Learning-Logik in einer Datenbank-Prozedur konsolidiert werden.
Prozedur und Calculation Views zur Auswertung k√∂nnen dann in den zentralen Produktions-Space deployed werden. Dort l√§sst sich die Cluster-Prozedur dann regelm√§√üig ausf√ºhren. Die Cluster-Ergebnisse stehen dann √ºber die Calculation Views direkt f√ºr die Konsumierung im Reporting zur Verf√ºgung.

Dank XSA l√§sst sich dieses Szenario auch in einem on-Premise-Umfeld ohne Data Sphere realisieren.

## Zusammenfassung

Das Zusammenfassen dieses Artikels √ºberlasse ich chat-GPT (Smart GPT-5) - mit kleineren Korrekturen:

* Das SQL-Notebook im SAP Business Application Studio erm√∂glicht interaktive Datenanalysen mit SQLScript und bietet gegen√ºber der klassischen SQL-Konsole den Vorteil, Ergebnisse mehrerer Befehle strukturiert zu speichern.
* Anhand eines Datensatzes fiktiver Kunden wurde gezeigt, wie eine Cluster-Analyse mit dem K-Means-Algorithmus durchgef√ºhrt wird und die Ergebnisse in einer Tabelle persistiert werden k√∂nnen.
* Erg√§nzend lassen sich statistische Berechnungen wie Percentile und Mittelwerte direkt in SQLScript ausf√ºhren, um die Cluster detaillierter zu charakterisieren. 
* Mit Calculation Views k√∂nnen die Ergebnisse anschlie√üend visuell aufbereitet werden, etwa durch Blasen- oder Streudiagramme sowie durch Techniken wie Binning zur Analyse von Altersgruppen. 
* Architektonisch spielen HDI-Container und Spaces in SAP HANA eine zentrale Rolle, da sie isolierte Umgebungen f√ºr Machine-Learning-Entwicklungen schaffen und den Zugriff auf produktive Daten sowie die Ressourcenkontrolle sicherstellen.



## Referenzen

[^churn]: Churn modeling von Kaggle: https://www.kaggle.com/datasets/shivan118/churn-modeling-dataset 
[^blog_calc_views]: https://www.brandeis.de/blog/2025-cluster-analysen-calculation-views/
[^thebook]: Mein Buch: https://es-tu.de/e9ZD2
[^git]: GitHub-Repository: https://github.com/drabap/SQLNotebookClustering 




